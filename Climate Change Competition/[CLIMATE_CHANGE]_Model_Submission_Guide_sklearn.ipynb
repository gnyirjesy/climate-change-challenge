{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[CLIMATE CHANGE] Model Submission Guide - sklearn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\"><img width=\"50%\" src=\"https://aimodelsharecontent.s3.amazonaws.com/aimodshare_banner.jpg\" /></p>\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "m_DMeZsXxpEZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXxGTgJz152A"
      },
      "source": [
        "# Climate Change Satellite Image Classification Competition Model Submission Guide - sklearn\n",
        "\n",
        "---\n",
        "**About the Original Data:**<br>\n",
        "*Data and Description accessed from [Tensorflow](https://www.tensorflow.org/datasets/catalog/bigearthnet)* <br>\n",
        "The BigEarthNet is a new large-scale Sentinel-2 benchmark archive, consisting of 590,326 Sentinel-2 image patches. The image patch size on the ground is 1.2 x 1.2 km with variable image size depending on the channel resolution. This is a multi-label dataset with 43 imbalanced labels, which has been simplified to single labels with 3 categories for the purposes of this competition.\n",
        "\n",
        "To construct the BigEarthNet, 125 Sentinel-2 tiles acquired between June 2017 and May 2018 over the 10 countries (Austria, Belgium, Finland, Ireland, Kosovo, Lithuania, Luxembourg, Portugal, Serbia, Switzerland) of Europe were initially selected. All the tiles were atmospherically corrected by the Sentinel-2 Level 2A product generation and formatting tool (sen2cor). Then, they were divided into 590,326 non-overlapping image patches. Each image patch was annotated by the multiple land-cover classes (i.e., multi-labels) that were provided from the CORINE Land Cover database of the year 2018 (CLC 2018).\n",
        "\n",
        "Bands and pixel resolution in meters:\n",
        "\n",
        "    B01: Coastal aerosol; 60m\n",
        "    B02: Blue; 10m\n",
        "    B03: Green; 10m\n",
        "    B04: Red; 10m\n",
        "    B05: Vegetation red edge; 20m\n",
        "    B06: Vegetation red edge; 20m\n",
        "    B07: Vegetation red edge; 20m\n",
        "    B08: NIR; 10m\n",
        "    B09: Water vapor; 60m\n",
        "    B11: SWIR; 20m\n",
        "    B12: SWIR; 20m\n",
        "    B8A: Narrow NIR; 20m\n",
        "\n",
        "License: Community Data License Agreement - Permissive, Version 1.0.\"\n",
        "\n",
        "**Competition Data Specifics:**<br>\n",
        "For the purpose of this competition, the original BigEarthNet dataset has been simplified to 20,000 images (15,000 training images and 5,000 test images) with 3 categories: \"forest\", \"nonforest\", and \"snow_shadow_cloud\", which contains images of snow and clouds. <br>\n",
        "Each \"image\" is a folder with 12 satellite image layers, each of which pics up on different features. The example preprocessor uses just three layers: B02, B03, and B04, which contain the standard RGB layers used in ML models. However, you are free to use any combination of the satellite image layers. \n",
        "\n",
        "**Data Source:**<br>\n",
        "Sumbul, G, Charfuelan, M, Demir, B and Markl, V. (2019). BigEarthNet: A Large-Scale Benchmark Archive For Remote Sensing Image Understanding. *Computing Research Repository (CoRR), abs/1902.06148.* https://www.tensorflow.org/datasets/catalog/bigearthnet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "---\n",
        "\n",
        "Let's share our models to a centralized leaderboard, so that we can collaborate and learn from the model experimentation process...\n",
        "\n",
        "**Instructions:**\n",
        "1.   Get data in and set up X_train / X_test / y_train\n",
        "2.   Preprocess data / Write and Save Preprocessor function\n",
        "3. Fit model on preprocessed data and save preprocessor function and model \n",
        "4. Generate predictions from X_test data and submit model to competition\n",
        "5. Repeat submission process to improve place on leaderboard\n",
        "\n"
      ],
      "metadata": {
        "id": "jt9GxUrX9dMC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gSrVJwp3E9H"
      },
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLTIaMB3ChSW",
        "outputId": "01be29fa-a7ae-4d65-aeff-5d47a6101b3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#install aimodelshare library\n",
        "! pip install aimodelshare-nightly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aimodelshare-nightly\n",
            "  Downloading aimodelshare_nightly-0.0.97-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting onnxmltools>=1.6.1\n",
            "  Downloading onnxmltools-1.10.0-py2.py3-none-any.whl (300 kB)\n",
            "\u001b[K     |████████████████████████████████| 300 kB 34.5 MB/s \n",
            "\u001b[?25hCollecting docker==5.0.0\n",
            "  Downloading docker-5.0.0-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 20.6 MB/s \n",
            "\u001b[?25hCollecting tf2onnx\n",
            "  Downloading tf2onnx-1.9.3-py3-none-any.whl (435 kB)\n",
            "\u001b[K     |████████████████████████████████| 435 kB 20.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from aimodelshare-nightly) (1.10.0+cu111)\n",
            "Collecting botocore==1.21.2\n",
            "  Downloading botocore-1.21.2-py3-none-any.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 17.0 MB/s \n",
            "\u001b[?25hCollecting onnxruntime>=1.7.0\n",
            "  Downloading onnxruntime-1.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 26.3 MB/s \n",
            "\u001b[?25hCollecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Collecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn>=0.11.2 in /usr/local/lib/python3.7/dist-packages (from aimodelshare-nightly) (0.11.2)\n",
            "Collecting PyJWT==2.2.0\n",
            "  Downloading PyJWT-2.2.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: xgboost>=0.90 in /usr/local/lib/python3.7/dist-packages (from aimodelshare-nightly) (0.90)\n",
            "Collecting shortuuid>=1.0.8\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting keras2onnx>=1.7.0\n",
            "  Downloading keras2onnx-1.7.0-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting boto3==1.18.2\n",
            "  Downloading boto3-1.18.2-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 37.9 MB/s \n",
            "\u001b[?25hCollecting onnxconverter-common>=1.7.0\n",
            "  Downloading onnxconverter_common-1.9.0-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from aimodelshare-nightly) (2.8.0)\n",
            "Collecting skl2onnx>=1.8.0\n",
            "  Downloading skl2onnx-1.11-py2.py3-none-any.whl (274 kB)\n",
            "\u001b[K     |████████████████████████████████| 274 kB 12.1 MB/s \n",
            "\u001b[?25hCollecting Pympler==0.9\n",
            "  Downloading Pympler-0.9.tar.gz (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 18.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from aimodelshare-nightly) (1.6.3)\n",
            "Collecting onnx>=1.9.0\n",
            "  Downloading onnx-1.11.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 41.7 MB/s \n",
            "\u001b[?25hCollecting urllib3==1.25.11\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse==1.6.3->aimodelshare-nightly) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse==1.6.3->aimodelshare-nightly) (0.37.1)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 446 kB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.21.2->aimodelshare-nightly) (2.8.2)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.3.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests!=2.18.0,>=2.14.2 in /usr/local/lib/python3.7/dist-packages (from docker==5.0.0->aimodelshare-nightly) (2.23.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->aimodelshare-nightly) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->aimodelshare-nightly) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->aimodelshare-nightly) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->aimodelshare-nightly) (1.1.0)\n",
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from keras2onnx>=1.7.0->aimodelshare-nightly) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.9.0->aimodelshare-nightly) (3.10.0.2)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=1.7.0->aimodelshare-nightly) (2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare-nightly) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare-nightly) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare-nightly) (3.0.4)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.11.2->aimodelshare-nightly) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.11.2->aimodelshare-nightly) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn>=0.11.2->aimodelshare-nightly) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn>=0.11.2->aimodelshare-nightly) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn>=0.11.2->aimodelshare-nightly) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn>=0.11.2->aimodelshare-nightly) (2018.9)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (13.0.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (0.24.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (2.8.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (1.44.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (1.13.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (57.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (1.1.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (1.0.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (0.5.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->aimodelshare-nightly) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.5.0->aimodelshare-nightly) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.5.0->aimodelshare-nightly) (3.2.0)\n",
            "Collecting flatbuffers\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: Pympler, wget, fire\n",
            "  Building wheel for Pympler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pympler: filename=Pympler-0.9-py3-none-any.whl size=164823 sha256=9192ebd1deb619dd9c3f742e84a9560aa6a688b97028b59bff4dd617f8420e54\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/f3/d8/35d5614ea4ddd295ffb9372a5f2f9570d9593d1ea4be33ec6d\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=e8f9269bf54f2fa1409d4d0ad1f8d081bcc5986f0686095d27dcf7e8342af3bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=74aca2da3bd01377e093cebe33b1447cefb304423a274229ace7f04b78022092\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built Pympler wget fire\n",
            "Installing collected packages: urllib3, onnx, jmespath, scikit-learn, onnxconverter-common, botocore, websocket-client, tf-estimator-nightly, skl2onnx, s3transfer, flatbuffers, fire, wget, tf2onnx, shortuuid, Pympler, PyJWT, onnxruntime, onnxmltools, keras2onnx, docker, boto3, aimodelshare-nightly\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed PyJWT-2.2.0 Pympler-0.9 aimodelshare-nightly-0.0.97 boto3-1.18.2 botocore-1.21.2 docker-5.0.0 fire-0.4.0 flatbuffers-1.12 jmespath-0.10.0 keras2onnx-1.7.0 onnx-1.11.0 onnxconverter-common-1.9.0 onnxmltools-1.10.0 onnxruntime-1.10.0 s3transfer-0.5.2 scikit-learn-0.24.2 shortuuid-1.0.8 skl2onnx-1.11 tf-estimator-nightly-2.8.0.dev2021122109 tf2onnx-1.9.3 urllib3-1.25.11 websocket-client-1.3.1 wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3PiJXBhC5y-",
        "outputId": "5d73bdd2-1246-47de-b6d8-454b84276d2e"
      },
      "source": [
        "# Get competition data - May take a couple minutes due to size of data set\n",
        "from aimodelshare import download_data\n",
        "download_data('public.ecr.aws/y2e2a1d6/climate_competition_data-repository:latest') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading [================================================>]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip Data - May take a couple minutes due to size of data set\n",
        "import zipfile\n",
        "with zipfile.ZipFile('climate_competition_data/climate_competition_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('competition_data')"
      ],
      "metadata": {
        "id": "oZIHmTo59tN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEzPoXPj3V7u"
      },
      "source": [
        "##2.   Preprocess data / Write and Save Preprocessor function\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up for data preprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "DC3T1mH89xTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is a pre-designed preprocessor, but you could also build your own to prepare the data differently\n",
        "\n",
        "def preprocessor(imageband_directory):\n",
        "        \"\"\"\n",
        "        This function preprocesses reads in images, resizes them to a fixed shape and\n",
        "        min/max transforms them before converting feature values to float32 numeric values\n",
        "        required by onnx files.\n",
        "        \n",
        "        params:\n",
        "            imageband_directory\n",
        "                path to folder with 13 satellite image bands\n",
        "                      \n",
        "        returns:\n",
        "            X\n",
        "                numpy array of preprocessed image data\n",
        "                  \n",
        "        \"\"\"\n",
        "           \n",
        "        import PIL\n",
        "        import os\n",
        "        import numpy as np\n",
        "        import tensorflow_datasets as tfds\n",
        "\n",
        "        def _load_tif(data):\n",
        "            \"\"\"Loads TIF file and returns as float32 numpy array.\"\"\"\n",
        "            img=tfds.core.lazy_imports.PIL_Image.open(data)\n",
        "            img = np.array(img.getdata()).reshape(img.size).astype(np.float32)\n",
        "            return img\n",
        "\n",
        "        image_list = []\n",
        "        filelist1=os.listdir(imageband_directory)\n",
        "        for fpath in filelist1:\n",
        "          fullpath=imageband_directory+\"/\"+fpath\n",
        "          if fullpath.endswith(('B02.tif','B03.tif','B04.tif')):\n",
        "              imgarray=_load_tif(imageband_directory+\"/\"+fpath)\n",
        "              image_list.append(imgarray)\n",
        "\n",
        "        X = np.stack(image_list,axis=2)   # to get (height,width,3)\n",
        "\n",
        "        X = np.expand_dims(X, axis=0) # Expand dims to add \"1\" to object shape [1, h, w, channels] for keras model.\n",
        "        X = np.array(X, dtype=np.float32) # Final shape for onnx runtime.\n",
        "        X=X/18581 # min max transform to max value\n",
        "        X = X.flatten()\n",
        "        return X"
      ],
      "metadata": {
        "id": "EyObJvH_9zO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create complete list of file names\n",
        "forestfilenames=[\"competition_data/trainingdata/forest/\"+x for x in os.listdir(\"competition_data/trainingdata/forest\")]\n",
        "nonforestfilenames=[\"competition_data/trainingdata/nonforest/\"+x for x in os.listdir(\"competition_data/trainingdata/nonforest\")]\n",
        "otherfilenames=[\"competition_data/trainingdata/other/\"+x for x in os.listdir(\"competition_data/trainingdata/other\")]\n",
        "\n",
        "filenames=forestfilenames+nonforestfilenames+otherfilenames\n",
        "\n",
        "#preprocess rbg images into 120,120,3 numpy ndarray, then flatten into a single 1 x 43200 row for each set of rbg images\n",
        "preprocessed_image_data=[]\n",
        "for i in filenames:\n",
        "  try:\n",
        "    preprocessed_image_data.append(preprocessor(i))\n",
        "  except:\n",
        "    pass  \n",
        "  "
      ],
      "metadata": {
        "id": "pT6B4SWs9038"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up y data\n",
        "from itertools import repeat\n",
        "forest=repeat(\"forest\",5000)\n",
        "nonforest=repeat(\"nonforest\",5000)\n",
        "other=repeat(\"snow_shadow_cloud\",5000)\n",
        "ylist=list(forest)+list(nonforest)+list(other)"
      ],
      "metadata": {
        "id": "9ASbrj_d92dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle X and y data\n",
        "from sklearn.utils import shuffle\n",
        "X_train, y_train = shuffle(preprocessed_image_data, ylist, random_state=0)"
      ],
      "metadata": {
        "id": "BWiaEYkZ93-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=np.vstack(X_train) # convert X from list to array"
      ],
      "metadata": {
        "id": "ri0zZQq-95aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ps2SWlK96xX",
        "outputId": "463922bf-d9a6-4b96-badf-d16a3a3903bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 43200)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess X_test Data: \n",
        "\n",
        "# import and preprocess X_test images in correct order...\n",
        "# ...for leaderboard prediction submissions\n",
        "filenumbers=[str(x) for x in range(1, 5001)]\n",
        "filenames=[\"competition_data/testdata/test/test\"+x for x in filenumbers]\n",
        "\n",
        "#preprocess rbg images into 120,120,3 numpy ndarray, then flatten into a single 1 x 43200 row for each set of rbg images\n",
        "preprocessed_image_data=[]\n",
        "for i in filenames:\n",
        "  try:\n",
        "    preprocessed_image_data.append(preprocessor(i))\n",
        "  except:\n",
        "    pass  "
      ],
      "metadata": {
        "id": "KQmHv_DEJppg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test=np.vstack(preprocessed_image_data) # convert X from list to array"
      ],
      "metadata": {
        "id": "7H5flnryJtG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X52kECL43b-O"
      },
      "source": [
        "##3. Fit model on preprocessed data and save preprocessor function and model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCbBf8j9ClYl",
        "outputId": "d2a5ea30-fffc-40e7-b150-71e33877ba73"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = RandomForestClassifier(n_estimators = 100, max_depth = 3, random_state=0)\n",
        "model.fit(X_train, y_train) # Fitting to the training set.\n",
        "prediction_labels = model.predict(X_train)\n",
        "\n",
        "f1_score(y_train, prediction_labels, average='macro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5649475514285425"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmJAnmO-5AcU"
      },
      "source": [
        "#### Save preprocessor function to local \"preprocessor.zip\" file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VGacc0LDaMA",
        "outputId": "150e0f17-6c7e-450c-f466-6a27ebef20e9"
      },
      "source": [
        "import aimodelshare as ai\n",
        "ai.export_preprocessor(preprocessor,\"\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "can't pickle module objects\n",
            "Your preprocessor is now saved to 'preprocessor.zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOWBa8Cv5LdL"
      },
      "source": [
        "#### Save model to local \".onnx\" file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEhvnRiQDlY5"
      },
      "source": [
        "# Save sklearn model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "# Check how many preprocessed input features are there?\n",
        "from skl2onnx.common.data_types import FloatTensorType\n",
        "\n",
        "feature_count=X_train.shape[1] #Get count of preprocessed features\n",
        "initial_type = [('float_input', FloatTensorType([None, feature_count]))]  #Insert correct number of preprocessed features\n",
        "\n",
        "onnx_model = model_to_onnx(model, framework='sklearn',\n",
        "                          initial_types=initial_type,\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=False)\n",
        "\n",
        "with open(\"model.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHWkAzvX3m8O"
      },
      "source": [
        "## 4. Generate predictions from X_test data and submit model to competition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtgkM02MDpkO",
        "outputId": "60baf57d-f434-42c3-a3fc-b7271720c0df"
      },
      "source": [
        "#Set credentials using modelshare.org username/password\n",
        "\n",
        "from aimodelshare.aws import set_credentials\n",
        "    \n",
        "apiurl=\"https://srdmat3yhf.execute-api.us-east-1.amazonaws.com/prod/m\"\n",
        " #This is the unique rest api that powers this Climate Change Satellite Image Classification Playground\n",
        "\n",
        "set_credentials(apiurl=apiurl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Modelshare Username:··········\n",
            "AI Modelshare Password:··········\n",
            "AI Model Share login credentials set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKNGSww8EGgi"
      },
      "source": [
        "#Instantiate Competition\n",
        "import aimodelshare as ai\n",
        "mycompetition= ai.Competition(apiurl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ql4wksyEUnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45acdf7-4990-4c27-8d78-3b081a577bbe"
      },
      "source": [
        "#Submit Model: \n",
        "\n",
        "#-- Generate predicted values (a list of predicted image categories) \n",
        "predicted_values = model.predict(X_test)\n",
        "\n",
        "# Submit model to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=predicted_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 2\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:1535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "GN1zvAmNEq17",
        "outputId": "f67ffb73-7aa1-49c8-9f72-ea1bf97219ef"
      },
      "source": [
        "# Get leaderboard to explore current best model architectures\n",
        "\n",
        "# Get raw data in pandas data frame\n",
        "data = mycompetition.get_leaderboard()\n",
        "\n",
        "# Stylize leaderboard data\n",
        "mycompetition.stylize_leaderboard(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_6cfa4_row0_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#f5f8d6 54.3%, transparent 54.3%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_6cfa4_row0_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#c778c8 46.1%, transparent 46.1%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_6cfa4_row0_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#ff4971 44.4%, transparent 44.4%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_6cfa4_row0_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#aadbaa 48.1%, transparent 48.1%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_6cfa4_row0_col4, #T_6cfa4_row0_col5, #T_6cfa4_row0_col6, #T_6cfa4_row0_col7, #T_6cfa4_row0_col8, #T_6cfa4_row0_col9, #T_6cfa4_row0_col10, #T_6cfa4_row0_col11, #T_6cfa4_row0_col12, #T_6cfa4_row0_col13, #T_6cfa4_row0_col14, #T_6cfa4_row0_col15, #T_6cfa4_row0_col16, #T_6cfa4_row0_col17, #T_6cfa4_row0_col18, #T_6cfa4_row0_col19, #T_6cfa4_row0_col20, #T_6cfa4_row0_col21, #T_6cfa4_row0_col22, #T_6cfa4_row1_col4, #T_6cfa4_row1_col5, #T_6cfa4_row1_col6, #T_6cfa4_row1_col7, #T_6cfa4_row1_col8, #T_6cfa4_row1_col9, #T_6cfa4_row1_col10, #T_6cfa4_row1_col11, #T_6cfa4_row1_col12, #T_6cfa4_row1_col13, #T_6cfa4_row1_col14, #T_6cfa4_row1_col15, #T_6cfa4_row1_col16, #T_6cfa4_row1_col17, #T_6cfa4_row1_col18, #T_6cfa4_row1_col19, #T_6cfa4_row1_col20, #T_6cfa4_row1_col21, #T_6cfa4_row1_col22 {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_6cfa4_row1_col0 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#f5f8d6 40.1%, transparent 40.1%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_6cfa4_row1_col1 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#c778c8 43.4%, transparent 43.4%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_6cfa4_row1_col2 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#ff4971 44.8%, transparent 44.8%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "#T_6cfa4_row1_col3 {\n",
              "  text-align: center;\n",
              "  width: 10em;\n",
              "  height: 80%;\n",
              "  background: linear-gradient(90deg,#aadbaa 47.8%, transparent 47.8%);\n",
              "  color: #251e1b;\n",
              "  font-size: 12px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_6cfa4_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >accuracy</th>\n",
              "      <th class=\"col_heading level0 col1\" >f1_score</th>\n",
              "      <th class=\"col_heading level0 col2\" >precision</th>\n",
              "      <th class=\"col_heading level0 col3\" >recall</th>\n",
              "      <th class=\"col_heading level0 col4\" >ml_framework</th>\n",
              "      <th class=\"col_heading level0 col5\" >transfer_learning</th>\n",
              "      <th class=\"col_heading level0 col6\" >deep_learning</th>\n",
              "      <th class=\"col_heading level0 col7\" >model_type</th>\n",
              "      <th class=\"col_heading level0 col8\" >depth</th>\n",
              "      <th class=\"col_heading level0 col9\" >num_params</th>\n",
              "      <th class=\"col_heading level0 col10\" >dropout_layers</th>\n",
              "      <th class=\"col_heading level0 col11\" >dense_layers</th>\n",
              "      <th class=\"col_heading level0 col12\" >flatten_layers</th>\n",
              "      <th class=\"col_heading level0 col13\" >conv2d_layers</th>\n",
              "      <th class=\"col_heading level0 col14\" >maxpooling2d_layers</th>\n",
              "      <th class=\"col_heading level0 col15\" >softmax_act</th>\n",
              "      <th class=\"col_heading level0 col16\" >relu_act</th>\n",
              "      <th class=\"col_heading level0 col17\" >loss</th>\n",
              "      <th class=\"col_heading level0 col18\" >optimizer</th>\n",
              "      <th class=\"col_heading level0 col19\" >model_config</th>\n",
              "      <th class=\"col_heading level0 col20\" >memory_size</th>\n",
              "      <th class=\"col_heading level0 col21\" >username</th>\n",
              "      <th class=\"col_heading level0 col22\" >version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_6cfa4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_6cfa4_row0_col0\" class=\"data row0 col0\" >54.28%</td>\n",
              "      <td id=\"T_6cfa4_row0_col1\" class=\"data row0 col1\" >46.12%</td>\n",
              "      <td id=\"T_6cfa4_row0_col2\" class=\"data row0 col2\" >44.40%</td>\n",
              "      <td id=\"T_6cfa4_row0_col3\" class=\"data row0 col3\" >48.11%</td>\n",
              "      <td id=\"T_6cfa4_row0_col4\" class=\"data row0 col4\" >sklearn</td>\n",
              "      <td id=\"T_6cfa4_row0_col5\" class=\"data row0 col5\" >False</td>\n",
              "      <td id=\"T_6cfa4_row0_col6\" class=\"data row0 col6\" >False</td>\n",
              "      <td id=\"T_6cfa4_row0_col7\" class=\"data row0 col7\" >RandomForestClassifier</td>\n",
              "      <td id=\"T_6cfa4_row0_col8\" class=\"data row0 col8\" >nan</td>\n",
              "      <td id=\"T_6cfa4_row0_col9\" class=\"data row0 col9\" >nan</td>\n",
              "      <td id=\"T_6cfa4_row0_col10\" class=\"data row0 col10\" >nan</td>\n",
              "      <td id=\"T_6cfa4_row0_col11\" class=\"data row0 col11\" >nan</td>\n",
              "      <td id=\"T_6cfa4_row0_col12\" class=\"data row0 col12\" >nan</td>\n",
              "      <td id=\"T_6cfa4_row0_col13\" class=\"data row0 col13\" >nan</td>\n",
              "      <td id=\"T_6cfa4_row0_col14\" class=\"data row0 col14\" >nan</td>\n",
              "      <td id=\"T_6cfa4_row0_col15\" class=\"data row0 col15\" >nan</td>\n",
              "      <td id=\"T_6cfa4_row0_col16\" class=\"data row0 col16\" >nan</td>\n",
              "      <td id=\"T_6cfa4_row0_col17\" class=\"data row0 col17\" >nan</td>\n",
              "      <td id=\"T_6cfa4_row0_col18\" class=\"data row0 col18\" >nan</td>\n",
              "      <td id=\"T_6cfa4_row0_col19\" class=\"data row0 col19\" >{'bootstrap': True, 'ccp_alpha...</td>\n",
              "      <td id=\"T_6cfa4_row0_col20\" class=\"data row0 col20\" >nan</td>\n",
              "      <td id=\"T_6cfa4_row0_col21\" class=\"data row0 col21\" >AIModelShare</td>\n",
              "      <td id=\"T_6cfa4_row0_col22\" class=\"data row0 col22\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6cfa4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_6cfa4_row1_col0\" class=\"data row1 col0\" >40.08%</td>\n",
              "      <td id=\"T_6cfa4_row1_col1\" class=\"data row1 col1\" >43.37%</td>\n",
              "      <td id=\"T_6cfa4_row1_col2\" class=\"data row1 col2\" >44.80%</td>\n",
              "      <td id=\"T_6cfa4_row1_col3\" class=\"data row1 col3\" >47.78%</td>\n",
              "      <td id=\"T_6cfa4_row1_col4\" class=\"data row1 col4\" >keras</td>\n",
              "      <td id=\"T_6cfa4_row1_col5\" class=\"data row1 col5\" >False</td>\n",
              "      <td id=\"T_6cfa4_row1_col6\" class=\"data row1 col6\" >True</td>\n",
              "      <td id=\"T_6cfa4_row1_col7\" class=\"data row1 col7\" >Sequential</td>\n",
              "      <td id=\"T_6cfa4_row1_col8\" class=\"data row1 col8\" >8.000000</td>\n",
              "      <td id=\"T_6cfa4_row1_col9\" class=\"data row1 col9\" >1847811.000000</td>\n",
              "      <td id=\"T_6cfa4_row1_col10\" class=\"data row1 col10\" >2.000000</td>\n",
              "      <td id=\"T_6cfa4_row1_col11\" class=\"data row1 col11\" >2.000000</td>\n",
              "      <td id=\"T_6cfa4_row1_col12\" class=\"data row1 col12\" >1.000000</td>\n",
              "      <td id=\"T_6cfa4_row1_col13\" class=\"data row1 col13\" >2.000000</td>\n",
              "      <td id=\"T_6cfa4_row1_col14\" class=\"data row1 col14\" >1.000000</td>\n",
              "      <td id=\"T_6cfa4_row1_col15\" class=\"data row1 col15\" >1.000000</td>\n",
              "      <td id=\"T_6cfa4_row1_col16\" class=\"data row1 col16\" >3.000000</td>\n",
              "      <td id=\"T_6cfa4_row1_col17\" class=\"data row1 col17\" >str</td>\n",
              "      <td id=\"T_6cfa4_row1_col18\" class=\"data row1 col18\" >RMSprop</td>\n",
              "      <td id=\"T_6cfa4_row1_col19\" class=\"data row1 col19\" >{'name': 'sequential', 'layers...</td>\n",
              "      <td id=\"T_6cfa4_row1_col20\" class=\"data row1 col20\" >2233032.000000</td>\n",
              "      <td id=\"T_6cfa4_row1_col21\" class=\"data row1 col21\" >AIModelShare</td>\n",
              "      <td id=\"T_6cfa4_row1_col22\" class=\"data row1 col22\" >1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f713be3a910>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwNKs0wP4r5s"
      },
      "source": [
        "## 5. Repeat submission process to improve place on leaderboard\n",
        "*Train and submit your own models using code modeled after what you see above.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tukB40NshcaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16611d80-80fc-4e7b-c3b9-47d3ca544ec9"
      },
      "source": [
        "# Here are several classic ML architectures you can consider choosing from to experiment with next:\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "#Example code to fit model:\n",
        "model = RandomForestClassifier(n_estimators = 300, max_depth = 2, random_state=0)\n",
        "model.fit(X_train, y_train) # Fitting to the training set.\n",
        "model.score(X_train, y_train)\n",
        "\n",
        "# Save sklearn model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "feature_count=X_test.shape[1] #Get count of preprocessed features\n",
        "initial_type = [('float_input', FloatTensorType([None, feature_count]))]  # Insert correct number of preprocessed features\n",
        "\n",
        "onnx_model = model_to_onnx(model, framework='sklearn',\n",
        "                          initial_types=initial_type,\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=False)\n",
        "\n",
        "with open(\"model.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())\n",
        "\n",
        "#-- Generate predicted values (a list of predicted labels \"real\" or \"fake\")\n",
        "prediction_labels = model.predict(X_test)\n",
        "\n",
        "# Submit model to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert search tags to help users find your model (optional): \n",
            "Provide any useful notes about your model (optional): \n",
            "\n",
            "Your model has been submitted as model version 3\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:1535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It may also be useful to examine the architeture of models that perform particuarly well/poorly, or to compare models you've created with similar models submitted by others. Use the compare_models function in combination with the leaderboard to learn more about models that been previously submitted and potentially make decisiona about what you should do next."
      ],
      "metadata": {
        "id": "yL5Im_H0UMFE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "qLl7yLpVEx26",
        "outputId": "9545898c-55b7-43e8-ff6e-b3f0848ce97b"
      },
      "source": [
        "# Compare two or more models\n",
        "data=mycompetition.compare_models([2, 3], verbose=1)\n",
        "mycompetition.stylize_compare(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_38e87_ caption {\n",
              "  color: black;\n",
              "  font-size: 18px;\n",
              "}\n",
              "#T_38e87_row4_col2, #T_38e87_row4_col3, #T_38e87_row13_col3, #T_38e87_row16_col2, #T_38e87_row16_col3 {\n",
              "  background: tomato;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_38e87_\">\n",
              "  <caption>Model type: RandomForestClassifier</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >param_name</th>\n",
              "      <th class=\"col_heading level0 col1\" >default_value</th>\n",
              "      <th class=\"col_heading level0 col2\" >model_version_2</th>\n",
              "      <th class=\"col_heading level0 col3\" >model_version_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_38e87_row0_col0\" class=\"data row0 col0\" >bootstrap</td>\n",
              "      <td id=\"T_38e87_row0_col1\" class=\"data row0 col1\" >True</td>\n",
              "      <td id=\"T_38e87_row0_col2\" class=\"data row0 col2\" >True</td>\n",
              "      <td id=\"T_38e87_row0_col3\" class=\"data row0 col3\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_38e87_row1_col0\" class=\"data row1 col0\" >ccp_alpha</td>\n",
              "      <td id=\"T_38e87_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
              "      <td id=\"T_38e87_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
              "      <td id=\"T_38e87_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_38e87_row2_col0\" class=\"data row2 col0\" >class_weight</td>\n",
              "      <td id=\"T_38e87_row2_col1\" class=\"data row2 col1\" >None</td>\n",
              "      <td id=\"T_38e87_row2_col2\" class=\"data row2 col2\" >None</td>\n",
              "      <td id=\"T_38e87_row2_col3\" class=\"data row2 col3\" >None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_38e87_row3_col0\" class=\"data row3 col0\" >criterion</td>\n",
              "      <td id=\"T_38e87_row3_col1\" class=\"data row3 col1\" >gini</td>\n",
              "      <td id=\"T_38e87_row3_col2\" class=\"data row3 col2\" >gini</td>\n",
              "      <td id=\"T_38e87_row3_col3\" class=\"data row3 col3\" >gini</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_38e87_row4_col0\" class=\"data row4 col0\" >max_depth</td>\n",
              "      <td id=\"T_38e87_row4_col1\" class=\"data row4 col1\" >None</td>\n",
              "      <td id=\"T_38e87_row4_col2\" class=\"data row4 col2\" >3</td>\n",
              "      <td id=\"T_38e87_row4_col3\" class=\"data row4 col3\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_38e87_row5_col0\" class=\"data row5 col0\" >max_features</td>\n",
              "      <td id=\"T_38e87_row5_col1\" class=\"data row5 col1\" >auto</td>\n",
              "      <td id=\"T_38e87_row5_col2\" class=\"data row5 col2\" >auto</td>\n",
              "      <td id=\"T_38e87_row5_col3\" class=\"data row5 col3\" >auto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_38e87_row6_col0\" class=\"data row6 col0\" >max_leaf_nodes</td>\n",
              "      <td id=\"T_38e87_row6_col1\" class=\"data row6 col1\" >None</td>\n",
              "      <td id=\"T_38e87_row6_col2\" class=\"data row6 col2\" >None</td>\n",
              "      <td id=\"T_38e87_row6_col3\" class=\"data row6 col3\" >None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_38e87_row7_col0\" class=\"data row7 col0\" >max_samples</td>\n",
              "      <td id=\"T_38e87_row7_col1\" class=\"data row7 col1\" >None</td>\n",
              "      <td id=\"T_38e87_row7_col2\" class=\"data row7 col2\" >None</td>\n",
              "      <td id=\"T_38e87_row7_col3\" class=\"data row7 col3\" >None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_38e87_row8_col0\" class=\"data row8 col0\" >min_impurity_decrease</td>\n",
              "      <td id=\"T_38e87_row8_col1\" class=\"data row8 col1\" >0.000000</td>\n",
              "      <td id=\"T_38e87_row8_col2\" class=\"data row8 col2\" >0.000000</td>\n",
              "      <td id=\"T_38e87_row8_col3\" class=\"data row8 col3\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_38e87_row9_col0\" class=\"data row9 col0\" >min_impurity_split</td>\n",
              "      <td id=\"T_38e87_row9_col1\" class=\"data row9 col1\" >None</td>\n",
              "      <td id=\"T_38e87_row9_col2\" class=\"data row9 col2\" >None</td>\n",
              "      <td id=\"T_38e87_row9_col3\" class=\"data row9 col3\" >None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_38e87_row10_col0\" class=\"data row10 col0\" >min_samples_leaf</td>\n",
              "      <td id=\"T_38e87_row10_col1\" class=\"data row10 col1\" >1</td>\n",
              "      <td id=\"T_38e87_row10_col2\" class=\"data row10 col2\" >1</td>\n",
              "      <td id=\"T_38e87_row10_col3\" class=\"data row10 col3\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "      <td id=\"T_38e87_row11_col0\" class=\"data row11 col0\" >min_samples_split</td>\n",
              "      <td id=\"T_38e87_row11_col1\" class=\"data row11 col1\" >2</td>\n",
              "      <td id=\"T_38e87_row11_col2\" class=\"data row11 col2\" >2</td>\n",
              "      <td id=\"T_38e87_row11_col3\" class=\"data row11 col3\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "      <td id=\"T_38e87_row12_col0\" class=\"data row12 col0\" >min_weight_fraction_leaf</td>\n",
              "      <td id=\"T_38e87_row12_col1\" class=\"data row12 col1\" >0.000000</td>\n",
              "      <td id=\"T_38e87_row12_col2\" class=\"data row12 col2\" >0.000000</td>\n",
              "      <td id=\"T_38e87_row12_col3\" class=\"data row12 col3\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "      <td id=\"T_38e87_row13_col0\" class=\"data row13 col0\" >n_estimators</td>\n",
              "      <td id=\"T_38e87_row13_col1\" class=\"data row13 col1\" >100</td>\n",
              "      <td id=\"T_38e87_row13_col2\" class=\"data row13 col2\" >100</td>\n",
              "      <td id=\"T_38e87_row13_col3\" class=\"data row13 col3\" >300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "      <td id=\"T_38e87_row14_col0\" class=\"data row14 col0\" >n_jobs</td>\n",
              "      <td id=\"T_38e87_row14_col1\" class=\"data row14 col1\" >None</td>\n",
              "      <td id=\"T_38e87_row14_col2\" class=\"data row14 col2\" >None</td>\n",
              "      <td id=\"T_38e87_row14_col3\" class=\"data row14 col3\" >None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "      <td id=\"T_38e87_row15_col0\" class=\"data row15 col0\" >oob_score</td>\n",
              "      <td id=\"T_38e87_row15_col1\" class=\"data row15 col1\" >False</td>\n",
              "      <td id=\"T_38e87_row15_col2\" class=\"data row15 col2\" >False</td>\n",
              "      <td id=\"T_38e87_row15_col3\" class=\"data row15 col3\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "      <td id=\"T_38e87_row16_col0\" class=\"data row16 col0\" >random_state</td>\n",
              "      <td id=\"T_38e87_row16_col1\" class=\"data row16 col1\" >None</td>\n",
              "      <td id=\"T_38e87_row16_col2\" class=\"data row16 col2\" >0</td>\n",
              "      <td id=\"T_38e87_row16_col3\" class=\"data row16 col3\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "      <td id=\"T_38e87_row17_col0\" class=\"data row17 col0\" >verbose</td>\n",
              "      <td id=\"T_38e87_row17_col1\" class=\"data row17 col1\" >0</td>\n",
              "      <td id=\"T_38e87_row17_col2\" class=\"data row17 col2\" >0</td>\n",
              "      <td id=\"T_38e87_row17_col3\" class=\"data row17 col3\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38e87_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "      <td id=\"T_38e87_row18_col0\" class=\"data row18 col0\" >warm_start</td>\n",
              "      <td id=\"T_38e87_row18_col1\" class=\"data row18 col1\" >False</td>\n",
              "      <td id=\"T_38e87_row18_col2\" class=\"data row18 col2\" >False</td>\n",
              "      <td id=\"T_38e87_row18_col3\" class=\"data row18 col3\" >False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}